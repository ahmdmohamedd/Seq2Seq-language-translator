{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e88e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695435a",
   "metadata": {},
   "source": [
    "# Step 2: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae7b339-ff30-4123-81f6-0c98ab357089",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dataset (assuming a CSV file with 'english' and 'french' columns)\n",
    "data = pd.read_csv('english-french.csv')\n",
    "english_sentences = data['english'].values\n",
    "french_sentences = data['french'].values\n",
    "\n",
    "# Tokenize the sentences\n",
    "eng_tokenizer = Tokenizer()\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "\n",
    "french_tokenizer = Tokenizer()\n",
    "french_tokenizer.fit_on_texts(french_sentences)\n",
    "french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "\n",
    "# Convert sentences to sequences of integers\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "french_sequences = french_tokenizer.texts_to_sequences(french_sentences)\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "max_eng_len = max([len(seq) for seq in eng_sequences])\n",
    "max_french_len = max([len(seq) for seq in french_sequences])\n",
    "\n",
    "eng_sequences = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
    "french_sequences = pad_sequences(french_sequences, maxlen=max_french_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2d2ef",
   "metadata": {},
   "source": [
    "# Define the Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdca27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "latent_dim = 256\n",
    "encoder_inputs = Input(shape=(max_eng_len,))\n",
    "encoder_embedding = Embedding(eng_vocab_size, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_french_len,))\n",
    "decoder_embedding = Embedding(french_vocab_size, latent_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(french_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Print the summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65d232",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7979e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift French sequences one time step to the right for decoder input\n",
    "french_sequences_input = french_sequences[:, :-1]\n",
    "french_sequences_output = french_sequences[:, 1:]\n",
    "\n",
    "# Reshape for sparse categorical cross-entropy\n",
    "french_sequences_output = np.expand_dims(french_sequences_output, -1)\n",
    "\n",
    "# Train the model\n",
    "model.fit([eng_sequences, french_sequences_input], french_sequences_output,\n",
    "          batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e537b",
   "metadata": {},
   "source": [
    "# Define the inference model for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Inference decoder model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc24c8",
   "metadata": {},
   "source": [
    "# Translate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(input_sentence):\n",
    "    # Tokenize and pad the input sentence\n",
    "    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')\n",
    "\n",
    "    # Get the encoder states\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence with only the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # Start translation\n",
    "    stop_condition = False\n",
    "    translated_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        # Predict next token\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Get the token index with highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = french_tokenizer.index_word.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_word == '' or sampled_word == '<eos>':\n",
    "            stop_condition = True\n",
    "        else\n",
    "            translated_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Update the target sequence and states\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    input_sentence = input(\"Enter an English sentence (or type 'exit' to quit): \")\n",
    "    if input_sentence.lower() == 'exit':\n",
    "        break\n",
    "    translation = translate_sentence(input_sentence)\n",
    "    print(f\"French translation: {translation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
